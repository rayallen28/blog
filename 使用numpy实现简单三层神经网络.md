# 使用numpy实现简单三层神经网络

```python
import numpy as np
# 激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
# 激活函数求导
def deriv_sigmoid(x):
    fx = sigmoid(x)
    return fx * (1 - fx)
# 优化目标定义（损失函数）
def mse_loss(y_true, y_pred):
    return ((y_true - y_pred) ** 2).mean()

# 三层神经网络——输入层-隐含层-输出层
class NeuroNetwork:
    def __init__(self):
        self.w1 = np.random.normal()
        self.w2 = np.random.normal()
        self.w3 = np.random.normal()
        self.w4 = np.random.normal()
        self.w5 = np.random.normal()
        self.w6 = np.random.normal()
        
        self.b1 = np.random.normal()
        self.b2 = np.random.normal()
        self.b3 = np.random.normal()
        
        self.h1 = Neuron(weights, bias)
        self.h2 = Neuron(weights, bias)
        self.o1 = Neuron(weights, bias)
    # 前向计算    
    def feedforward(self, x):
        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)
        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)
        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)
        return o1
    # 训练
    def train(self, data, all_y_trues):
        # 学习率
        learning_rate = 0.1
        # 训练轮次
        epochs = 1000
        for epoch in range(epochs):
            for x, y_true in zip(data, all_y_trues):
                # 临时保存前向运算中间结果
                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1
                h1 = sigmoid(sum_h1)
                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2
                h2 = sigmoid(sum_h2)
                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3
                o1 = sigmoid(sum_o1)
                y_pred = o1
                # 链式求导——反向传播
                d_L_d_ypred = -2 * (y_true - y_pred)
                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)
                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)
                d_ypred_d_b3 = deriv_sigmoid(sum_o1)
                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)
                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)
                
                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)
                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)
                d_h1_d_b1 = deriv_sigmoid(sum_h1)
                
                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)
                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)
                d_h2_d_b2 = deriv_sigmoid(sum_h2)
                # 更新参数——随机梯度下降SGD
                self.w1 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1
                self.w2 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2
                self.b1 -= learning_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1
                
                self.w3 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3
                self.w4 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4
                self.b2 -= learning_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2
                
                self.w5 -= learning_rate * d_L_d_ypred * d_ypred_d_w5
                self.w6 -= learning_rate * d_L_d_ypred * d_ypred_d_w6
                self.b3 -= learning_rate * d_L_d_ypred * d_ypred_d_b3
                # 跟踪LOSS
                if epoch % 10 == 0:
                    y_preds = np.apply_along_axis(self.feedforward, 1, data)
                    loss = mse_loss(all_y_trues, y_preds)
                    print(f'Epoch {epoch} loss: {loss}')
# 导入训练数据
data = np.array([[-2, -1], [25, 6], [17, 4], [-15, -6]])
all_y_trues = np.array([1, 0, 0, 1])
# 训练模型
network = NeuroNetwork()
network.train(data, all_y_trues)
```
训练过程输出：
```text
Epoch 0 loss: 0.343475459791441
Epoch 0 loss: 0.343531516462779
Epoch 0 loss: 0.3436367479901792
Epoch 0 loss: 0.3389017222763815
Epoch 10 loss: 0.2514999471350128
Epoch 10 loss: 0.2516083574369558
Epoch 10 loss: 0.2517829949315882
Epoch 10 loss: 0.2462108512554566
Epoch 20 loss: 0.17297660777910967
Epoch 20 loss: 0.17270269056693818
Epoch 20 loss: 0.17251533230230448
Epoch 20 loss: 0.1691459995864174
Epoch 30 loss: 0.12290663929762366
Epoch 30 loss: 0.1224382974608919
Epoch 30 loss: 0.12204600826550652
Epoch 30 loss: 0.12044464904761976
Epoch 40 loss: 0.09110086408429295
Epoch 40 loss: 0.09066483928060834
Epoch 40 loss: 0.09028223087706194
Epoch 40 loss: 0.08947607059840013
Epoch 50 loss: 0.06985592655822194
Epoch 50 loss: 0.06951194717265871
Epoch 50 loss: 0.06920252967363746
Epoch 50 loss: 0.06875227015250464
Epoch 60 loss: 0.05517681535000449
Epoch 60 loss: 0.054917314099792336
Epoch 60 loss: 0.054679840795069795
Epoch 60 loss: 0.05440520286821526
Epoch 70 loss: 0.044756039265352425
Epoch 70 loss: 0.04456096895184733
Epoch 70 loss: 0.0443801333991553
Epoch 70 loss: 0.04420048374223778
Epoch 80 loss: 0.03716150486006733
Epoch 80 loss: 0.03701318456694737
Epoch 80 loss: 0.036874298093726804
Epoch 80 loss: 0.03675002522547959
Epoch 90 loss: 0.03148250123690647
Epoch 90 loss: 0.031367838013257585
Epoch 90 loss: 0.03125960842007858
Epoch 90 loss: 0.031169673291214027
Epoch 100 loss: 0.027131679726862167
Epoch 100 loss: 0.0270414469330036
Epoch 100 loss: 0.026955727876143632
Epoch 100 loss: 0.026888207483137992
Epoch 110 loss: 0.023723872780612516
Epoch 110 loss: 0.023651632414006696
Epoch 110 loss: 0.023582643910912
Epoch 110 loss: 0.023530399812977235
Epoch 120 loss: 0.021001359888998445
Epoch 120 loss: 0.02094259304438063
Epoch 120 loss: 0.020886226206450722
Epoch 120 loss: 0.02084477875442228
Epoch 130 loss: 0.018787888861393372
Epoch 130 loss: 0.018739383255912158
Epoch 130 loss: 0.018692687803339218
Epoch 130 loss: 0.018659109835951992
Epoch 140 loss: 0.0169602448137153
Epoch 140 loss: 0.016919681695437092
Epoch 140 loss: 0.01688051058192186
Epoch 140 loss: 0.016852821461965903
Epoch 150 loss: 0.015430449474667603
Epoch 150 loss: 0.015396127987783902
Epoch 150 loss: 0.015362895643236405
Epoch 150 loss: 0.015339714043714973
Epoch 160 loss: 0.01413442561242911
Epoch 160 loss: 0.014105078258140237
Epoch 160 loss: 0.014076596362201682
Epoch 160 loss: 0.014056933218722166
Epoch 170 loss: 0.013024639570028952
Epoch 170 loss: 0.012999307418197482
Epoch 170 loss: 0.012974672572039218
Epoch 170 loss: 0.012957803172983194
Epoch 180 loss: 0.012065231102495758
Epoch 180 loss: 0.012043178301910921
Epoch 180 loss: 0.01202169435801443
Epoch 180 loss: 0.012007076801230603
Epoch 190 loss: 0.011228726698486501
Epoch 190 loss: 0.011209381068927281
Epoch 190 loss: 0.011190504739666316
Epoch 190 loss: 0.011177726471452295
Epoch 200 loss: 0.010493779986570412
Epoch 200 loss: 0.010476691131438136
Epoch 200 loss: 0.01045999338795175
Epoch 200 loss: 0.010448735264444224
Epoch 210 loss: 0.009843590764118865
Epoch 210 loss: 0.009828400111805226
Epoch 210 loss: 0.009813538420775859
Epoch 210 loss: 0.0098035499996817
Epoch 220 loss: 0.00926478049318872
Epoch 220 loss: 0.009251199573340767
Epoch 220 loss: 0.009237897667033233
Epoch 220 loss: 0.009228979873193875
Epoch 230 loss: 0.008746580102183892
Epoch 230 loss: 0.008734374585618567
Epoch 230 loss: 0.008722407535639458
Epoch 230 loss: 0.00871440027537352
Epoch 240 loss: 0.008280234921585115
Epoch 240 loss: 0.008269212748539466
Epoch 240 loss: 0.008258395821038622
Epoch 240 loss: 0.008251168984395168
Epoch 250 loss: 0.0078585628802985
Epoch 250 loss: 0.007848565326149097
Epoch 250 loss: 0.00783874556627624
Epoch 250 loss: 0.0078321923909935
Epoch 260 loss: 0.007475622418845874
Epoch 260 loss: 0.007466517296383632
Epoch 260 loss: 0.007457567105822861
Epoch 260 loss: 0.007451599215078179
Epoch 270 loss: 0.00712645999278468
Epoch 270 loss: 0.00711813642663979
Epoch 270 loss: 0.007109948612979587
Epoch 270 loss: 0.007104492257507898
Epoch 280 loss: 0.00680691603046754
Epoch 280 loss: 0.0067992804011381015
Epoch 280 loss: 0.0067917643257498295
Epoch 280 loss: 0.006786757504718556
Epoch 290 loss: 0.006513474321446335
Epoch 290 loss: 0.006506447088514318
Epoch 290 loss: 0.006499525642588202
Epoch 290 loss: 0.0064949158739385495
Epoch 300 loss: 0.006243144024254551
Epoch 300 loss: 0.0062366572169730745
Epoch 300 loss: 0.0062302644278601545
Epoch 300 loss: 0.006226007004386631
Epoch 310 loss: 0.005993366423152167
Epoch 310 loss: 0.005987361642309078
Epoch 310 loss: 0.0059814407639161215
Epoch 310 loss: 0.005977497376651898
Epoch 320 loss: 0.005761940641693535
Epoch 320 loss: 0.0057563674565859774
Epoch 320 loss: 0.005750869434676737
Epoch 320 loss: 0.0057472070768840415
Epoch 330 loss: 0.005546964006659571
Epoch 330 loss: 0.0055417786599232
Epoch 330 loss: 0.005536660890131322
Epoch 330 loss: 0.0055332509777129574
Epoch 340 loss: 0.005346783829536754
Epoch 340 loss: 0.005341948183708803
Epoch 340 loss: 0.005337173499829765
Epoch 340 loss: 0.005333991160149603
Epoch 350 loss: 0.005159958156586743
Epoch 350 loss: 0.0051554388318788245
Epoch 350 loss: 0.005150974676631408
Epoch 350 loss: 0.005147998169477373
Epoch 360 loss: 0.004985223616412288
Epoch 360 loss: 0.004980991280780184
Epoch 360 loss: 0.004976809022140982
Epoch 360 loss: 0.004974019265185628
Epoch 370 loss: 0.004821468923836085
Epoch 370 loss: 0.004817497705151316
Epoch 370 loss: 0.004813572069578156
Epoch 370 loss: 0.004810952247082095
Epoch 380 loss: 0.0046677129215319385
Epoch 380 loss: 0.0046639799182572415
Epoch 380 loss: 0.004660288518345397
Epoch 380 loss: 0.004657823756630593
Epoch 390 loss: 0.004523086284950335
Epoch 390 loss: 0.004519571156752002
Epoch 390 loss: 0.004516094095625757
Epoch 390 loss: 0.004513771192454116
Epoch 400 loss: 0.004386816202214011
Epoch 400 loss: 0.0043835008258082085
Epoch 400 loss: 0.004380220364212221
Epoch 400 loss: 0.004378027561950827
Epoch 410 loss: 0.0042582134836413165
Epoch 410 loss: 0.00425508166216019
Epoch 410 loss: 0.004251981937052246
Epoch 410 loss: 0.004249908731612569
Epoch 420 loss: 0.0041366616661628755
Epoch 420 loss: 0.004133698882652944
Epoch 420 loss: 0.0041307656683115595
Epoch 420 loss: 0.004128802647477008
Epoch 430 loss: 0.004021607764031563
Epoch 430 loss: 0.004018800971519645
Epoch 430 loss: 0.004016021475916086
Epoch 430 loss: 0.0040141601819376205
Epoch 440 loss: 0.003912554384730463
Epoch 440 loss: 0.003909891826728791
Epoch 440 loss: 0.00390725451728259
Epoch 440 loss: 0.003905487329613672
Epoch 450 loss: 0.003809052982207202
Epoch 450 loss: 0.003806524038668048
Epoch 450 loss: 0.003804018492592435
Epoch 450 loss: 0.0038023385274152223
Epoch 460 loss: 0.003710698061770129
Epoch 460 loss: 0.0037082931164076445
Epoch 460 loss: 0.00370590989171889
Epoch 460 loss: 0.0037043109155330644
Epoch 470 loss: 0.0036171221846376888
Epoch 470 loss: 0.0036148325102616
Epoch 470 loss: 0.003612563034222057
Epoch 470 loss: 0.0036110393892611124
Epoch 480 loss: 0.003527991647109883
Epoch 480 loss: 0.0035258093062009816
Epoch 480 loss: 0.0035236457785265624
Epoch 480 loss: 0.003522192318162141
Epoch 490 loss: 0.003443002731065359
Epoch 490 loss: 0.003440920489296283
Epoch 490 loss: 0.0034388557979132504
Epoch 490 loss: 0.0034374678305268865
Epoch 500 loss: 0.003361878440081554
Epoch 500 loss: 0.0033598896908710314
Epoch 500 loss: 0.0033579173383768432
Epoch 500 loss: 0.0033565905784365174
Epoch 510 loss: 0.003284365649784074
Epoch 510 loss: 0.003282464348287208
Epoch 510 loss: 0.0032805783875717244
Epoch 510 loss: 0.0032793089128608822
Epoch 520 loss: 0.0032102326127197794
Epoch 520 loss: 0.0032084132179142346
Epoch 520 loss: 0.0032066081956457337
Epoch 520 loss: 0.0032053924097636462
Epoch 530 loss: 0.0031392667676357023
Epoch 530 loss: 0.0031375241913757556
Epoch 530 loss: 0.003135795098260285
Epoch 530 loss: 0.0031346296976537606
Epoch 540 loss: 0.003071272810943796
Epoch 540 loss: 0.003069602373029426
Epoch 540 loss: 0.0030679445999205494
Epoch 540 loss: 0.0030668265448230148
Epoch 550 loss: 0.0030060709946815338
Epoch 550 loss: 0.0030044683831348755
Epoch 550 loss: 0.003002877682211477
Epoch 550 loss: 0.003001804170961362
Epoch 560 loss: 0.0029434956206989046
Epoch 560 loss: 0.002941956856561489
Epoch 560 loss: 0.00294042930690776
Epoch 560 loss: 0.002939397753198006
Epoch 570 loss: 0.002883393705317522
Epoch 570 loss: 0.0028819151113826744
Epoch 570 loss: 0.0028804470884020938
Epoch 570 loss: 0.0028794551010790288
Epoch 580 loss: 0.0028256237924820377
Epoch 580 loss: 0.0028242019654615156
Epoch 580 loss: 0.002822790113638516
Epoch 580 loss: 0.0028218354787240287
Epoch 590 loss: 0.0027700548965897357
Epoch 590 loss: 0.0027686866822849484
Epoch 590 loss: 0.0027673278908768883
Epoch 590 loss: 0.0027664085555344268
Epoch 600 loss: 0.002716565558848272
Epoch 600 loss: 0.0027152480299564206
Epoch 600 loss: 0.002713939411256534
Epoch 600 loss: 0.002713053469460865
Epoch 610 loss: 0.0026650430032597225
Epoch 610 loss: 0.0026637734394960415
Epoch 610 loss: 0.0026625123093574196
Epoch 610 loss: 0.0026616579890612765
Epoch 620 loss: 0.0026153823802331213
Epoch 620 loss: 0.002614158250493626
Epoch 620 loss: 0.002612942110845954
Epoch 620 loss: 0.0026121177624647777
Epoch 630 loss: 0.0025674860874440202
Epoch 630 loss: 0.0025663050337697284
Epoch 630 loss: 0.002565131556896114
Epoch 630 loss: 0.002564335642956123
Epoch 640 loss: 0.002521263158935835
Epoch 640 loss: 0.002520122982070812
Epoch 640 loss: 0.002518989996442573
Epoch 640 loss: 0.0025182210822577026
Epoch 650 loss: 0.002476628714633163
Epoch 650 loss: 0.002475527360995543
Epoch 650 loss: 0.0024744328384885893
Epoch 650 loss: 0.0024736895837493183
Epoch 660 loss: 0.0024335034634431307
Epoch 660 loss: 0.0024324390133512504
Epoch 660 loss: 0.0024313810576902318
Epoch 660 loss: 0.002430662208862168
Epoch 670 loss: 0.0023918132539842672
Epoch 670 loss: 0.002390783910999739
Epoch 670 loss: 0.0023897607472952026
Epoch 670 loss: 0.002389065130738243
Epoch 680 loss: 0.002351488667725319
Epoch 680 loss: 0.0023504927489919995
Epoch 680 loss: 0.0023495027142524617
Epoch 680 loss: 0.002348829229982172
Epoch 690 loss: 0.002312464649956979
Epoch 690 loss: 0.002311500577429406
Epoch 690 loss: 0.0023105421119444464
Epoch 690 loss: 0.002309889727967155
Epoch 700 loss: 0.002274680174573402
Epoch 700 loss: 0.0022737464670410765
Epoch 700 loss: 0.002272818106544013
Epoch 700 loss: 0.002272185853705064
Epoch 710 loss: 0.00223807793911993
Epoch 710 loss: 0.0022371732049448756
Epoch 710 loss: 0.0022362735734744324
Epoch 710 loss: 0.002235660540766327
Epoch 720 loss: 0.0022026040869804592
Epoch 720 loss: 0.0022017270174750306
Epoch 720 loss: 0.0022008548208645447
Epoch 720 loss: 0.002200260151147905
Epoch 730 loss: 0.0021682079539396843
Epoch 730 loss: 0.002167357317320102
Epoch 730 loss: 0.0021665113372512515
Epoch 730 loss: 0.0021659342233470405
Epoch 740 loss: 0.0021348418366722243
Epoch 740 loss: 0.002134016472530429
Epoch 740 loss: 0.0021331955610953838
Epoch 740 loss: 0.002132635242211296
Epoch 750 loss: 0.0021024607809859656
Epoch 750 loss: 0.0021016595952290857
Epoch 750 loss: 0.002100862669951324
Epoch 750 loss: 0.0021003184284096575
Epoch 760 loss: 0.0020710223878892425
Epoch 760 loss: 0.002070244348101151
Epoch 760 loss: 0.0020694703873706524
Epoch 760 loss: 0.002068941545608468
Epoch 770 loss: 0.0020404866357628592
Epoch 770 loss: 0.00203973076694774
Epoch 770 loss: 0.0020389788058310116
Epoch 770 loss: 0.00203846472364655
Epoch 780 loss: 0.0020108157171045507
Epoch 780 loss: 0.0020100810977762115
Epoch 780 loss: 0.0020093502241659423
Epoch 780 loss: 0.0020088502961880944
Epoch 790 loss: 0.001981973888477087
Epoch 790 loss: 0.0019812596470619856
Epoch 790 loss: 0.0019805489981346938
Epoch 790 loss: 0.0019800626514947842
Epoch 800 loss: 0.00195392733243585
Epoch 800 loss: 0.0019532326439608717
Epoch 800 loss: 0.0019525414029143953
Epoch 800 loss: 0.0019520680951017144
Epoch 810 loss: 0.0019266440303391044
Epoch 810 loss: 0.001925968113378349
Epoch 810 loss: 0.0019252955064237571
Epoch 810 loss: 0.0019248347233081606
Epoch 820 loss: 0.0019000936450573717
Epoch 820 loss: 0.0018994357589146138
Epoch 820 loss: 0.001898781052499837
Epoch 820 loss: 0.0018983323065064877
Epoch 830 loss: 0.0018742474126979254
Epoch 830 loss: 0.0018736068548040091
Epoch 830 loss: 0.001872969353048667
Epoch 830 loss: 0.001872532181471439
Epoch 840 loss: 0.001849078042549542
Epoch 840 loss: 0.0018484541460557834
Epoch 840 loss: 0.0018478331883788222
Epoch 840 loss: 0.0018474071518202685
Epoch 850 loss: 0.001824559624531211
Epoch 850 loss: 0.0018239517560819038
Epoch 850 loss: 0.0018233467150054212
Epoch 850 loss: 0.0018229313959324
Epoch 860 loss: 0.0018006675434989418
Epoch 860 loss: 0.0018000751011677436
Epoch 860 loss: 0.0017994853802819066
Epoch 860 loss: 0.0017990803816870064
Epoch 870 loss: 0.0017773783998272018
Epoch 870 loss: 0.0017768008112034254
Epoch 870 loss: 0.0017762258432790215
Epoch 870 loss: 0.001775830787438844
Epoch 880 loss: 0.0017546699357374274
Epoch 880 loss: 0.0017541066561498836
Epoch 880 loss: 0.0017535459013861081
Epoch 880 loss: 0.0017531604287083884
Epoch 890 loss: 0.0017325209668961343
Epoch 890 loss: 0.0017319714777630313
Epoch 890 loss: 0.0017314244221594756
Epoch 890 loss: 0.0017310481901117124
Epoch 900 loss: 0.001710911318849553
Epoch 900 loss: 0.0017103751261442809
Epoch 900 loss: 0.001709841279986955
Epoch 900 loss: 0.0017094739620998908
Epoch 910 loss: 0.0016898217679019896
Epoch 910 loss: 0.001689298400725407
Epoch 910 loss: 0.001688777297177615
Epoch 910 loss: 0.0016884185821176398
Epoch 920 loss: 0.0016692339860809899
Epoch 920 loss: 0.0016687229953317086
Epoch 920 loss: 0.001668214189121472
Epoch 920 loss: 0.0016678637798263247
Epoch 930 loss: 0.0016491304898646093
Epoch 930 loss: 0.0016486314469995387
Epoch 930 loss: 0.0016481345131959583
Epoch 930 loss: 0.0016477921260688844
Epoch 940 loss: 0.0016294945923752946
Epoch 940 loss: 0.0016290070882534606
Epoch 940 loss: 0.0016285216211250474
Epoch 940 loss: 0.0016281869852827978
Epoch 950 loss: 0.0016103103587708845
Epoch 950 loss: 0.0016098340025740225
Epoch 950 loss: 0.0016093596145227572
Epoch 950 loss: 0.0016090324710932827
Epoch 960 loss: 0.0015915625645869476
Epoch 960 loss: 0.0015910969828110545
Epoch 960 loss: 0.0015906333033763338
Epoch 960 loss: 0.0015903134048424362
Epoch 970 loss: 0.0015732366568059273
Epoch 970 loss: 0.0015727814923183797
Epoch 970 loss: 0.0015723281672456206
Epoch 970 loss: 0.001572015276831037
Epoch 980 loss: 0.0015553187174477756
Epoch 980 loss: 0.0015548736286051627
Epoch 980 loss: 0.0015544303189742345
Epoch 980 loss: 0.0015541242100690581
Epoch 990 loss: 0.001537795429494369
Epoch 990 loss: 0.0015373600893164557
Epoch 990 loss: 0.001536926470725511
Epoch 990 loss: 0.0015366269263479857
```
```python
# 推理
test1 = np.array([-7, -3])
print(f'test1 is {network.feedforward(test1)}')
test2 = np.array([20, 2])
print(f'test1 is {network.feedforward(test2)}')
```
输出：
```text
test1 is 0.9669386740975608
test2 is 0.03907260671447344
```

参考：[https://victorzhou.com/blog/intro-to-neural-networks/]
